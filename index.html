<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
<style>
img[alt=archpic]{height:50%;width:85%;}
img[alt=archpic2]{height:50%;width:50%;}

.reveal section p {
    font-size: 0.75em !important;
}

li {
    font-size: 0.8em;
}
</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown data-background-image="res/bg/soundbendor-logo.png" data-background-position="100% 92.5%" data-background-size="200px">
                    <textarea data-template>
                        ## Music Source Separation in the Waveform Domain (2021)
                        Alexandre Défossez (FAIR), Nicolas Usunier (FAIR), Léon Bottou (FAIR), Francis Bach (INRIA)



                        Presentation by Derek Kwan - November 6, 2023
                        ---
                        ## Problem Statement
                        - want to "demix"/"demux" a music recording into its constituent instrumental parts
                            - DAW: instrumental tracks/"stems" (for remixing)
                        - "cocktail party"
                        - difficult because of overtones (among other things)
                        - uses rock band-ish instrumentation (vocal, drums, bass, other)
                            - understandable since standardized, number of instruments small, and relatively distinguishable
                                - compare with orchestra or string quartet (2 violins, viola, cello)
                        - SiSec Mus (Inria), Sound DemiXing Challenge (Sony/Mitsubishi)
                        ---
                        ## Problem Setup
                        - Approach: Supervised
                        - Dataset: MusDB
                            - mixed-down music tracks
                            - separate instrument tracks
                            - https://sigsep.github.io/datasets/musdb.html (requestable, manual approval)
                        ---
                        ## Terms
                        - Mask (S = Signal, N = Noise, Y = Mix) Baselines
                            - calculated using speech and noise, reconstruction using noisy phase
                            - Ideal Binary Mask (IBM)
                                - based on "masking phenomenon of human auditory system" (Wang et al., 2016)
                                - 1 if SNR > LC, else 0
                            - Ideal Ratio Mask (IRM)
                                - `$$ (\frac{|S|^2}{|S|^2 + |N|^2})^\beta $$`
                        ---
                        ## Terms Cont.
                        - SDR (signal to distortion ratio, interf = interference, artif = artifacts) (Vincent et al, 2006)
                            - `$$ 10 \log_{10}{\frac{\lVert s_{target} \rVert ^2}{\lVert e_{interf} + e_{artif} \rVert ^2}} $$`
                        - PReLu
                            - `$$ max(0,x) + a * min(0,x) $$`
                        - Gated Linear Unit (GLU)
                            - like LSTM, passes info through
                        - STFT - Short-Time Fourier Transform
                            - FT on successive windows (to get spectrograms)
                        ---
                        ## Related Work (non-DL)
                        - non-negative matrix factorization
                        - independent component analysis (ICA)
                        - Hidden Markov Models
                        - segmentation
                        ---
                        ## Related Work (DL)
                        - typically spectrogram (STFT) based
                            - has used fully-connected networks
                            - convolutional
                                - Conv-Tasnet - monophonic speech: 8 kHz SR
                                    - outperforms IBM, IRM
                                - D3Net
                                    - dilated convolutions
                        - Raw Waveform
                            - Wave U-Net
                                - originally spectrogram, converted for raw waveforms
                                - raw waveform performance not as good as spectrogram methods
                        
                        ---
                        ## Conv-Tasnet
                        - minimize `$$ \min_{\theta} \sum_{x \in \mathcal{D}} \sum_{s=1}^{S} L(g_s(x;\theta), x_s) $$`
                        - C = channels, T = length of waveform in samples, D = dataset, L = reconstruction error
                        - source signal index s `$$ x_s \in \mathbb{R}^{C,T} $$` 
                        - g<sub>s</sub> model outputting s with parameters x (input) and theta (parameters)
                        ---
                        ## Conv-Tasnet
                        - monophonic 8 kHZ to 128 channels at 1 kHz
                        - convolutional encoder (H - nonlinearity, x<sub>k</sub> - 1 x L real input (k = 1..T segments of input), U - N (vectors) x L basis functions, w - 1 X N real output )
                            - `$$ \bm{w} = \mathcal{H}(\bm{x} \bm{U}) $$`
                            - input: signal
                            - kernel size/stride = 16/8
                            - blocks uses depth-wise separable convolutions (3/1), dilated convolutions (2<sup>n mod N</sup> , n 0-index of block, N = 8) for temporal connections (opposed to RNN)
                            - outputs mask (sum + ReLU)
                        ---
                        ## Conv-Tasnet
                        - transposed convolutional decoder (V - N x L decoder basis rows)
                            - `$$ \bm{\hat{x}} = \bm{w} \bm{V} $$`
                            - input = mask * signal
                            - kernel size/stride = 16/8
                            - outputs reconstructed signal (overlap reconstructions and add)
                        ---
                        ## Architecture (Conv-Tasnet)
                        ![archpic](res/convtasnet.png)
                        ---
                        ## Conv-Tasnet Adaptation
                        - originally
                            - 3 X N=8 dilated conv blocks
                            - 8 kHZ mono to 1 kHz and 128 channels encoded (receptive field: 1.5 sec)
                            - 16/8 ks/str
                            - seconds long snippets of speech
                        - adapted
                            - 4 x N=10 dilated conv blocks (same receptive field at 44.1 kHz)
                            - 256 channels
                            - 20/10 ks/str
                            - 8 seconds of music (not too long because of normalization concerns)

                        ---
                        ## Demucs Overview
                        - architecture
                            - conv encoder, bidirectional LSTM, conv decoder
                                - (superficially) like Conv-Tasnet but replacing dilated conv with LSTM
                                - "represent masks on a learnt representation" (Défossez et al, 2021)
                                    - "more expressive than Conv-Tasnet" (Défossez et al, 2021)
                                    - latent representation a little hairier (since input is raw waveforms so mutliplication analogue to Conv-Tasnet doesn't really work)
                            - U-Net (residual connections between ith encoder conv and L-i-1th decoder conv layer)
                                - Conv-Tasnet has residual connections but only within encoder/decoder
                                - pairing encoded representations with decoded
                                    - share phase information
                        - upsamples input 2x using sinc, downsamples 2x,
                        ---
                        ## Architecture (Demucs)
                        ![archpic2](res/demucs-whole.png)
                        ---

                        ## Demucs Overview
                        - loss functions (T = samples, x<sub>s,t</sub> = input waveform s at sample t, x_hat<sub>s,t</sub> = output waveform s at sample t):
                            - `$$ L_1(\hat{x_s}, x_s) = \frac{1}{T} \sum_{t=1}^T |\hat{x_{s,t}} - x_{s,t}| $$`
                            - `$$ L_2(\hat{x_s}, x_s) = \frac{1}{T} \sum_{t=1}^T (\hat{x_{s,t}} - x_{s,t})^2 $$`

                            - usually a problem because of phase differences
                            - sound separation uses original phase (so fine)
                            - no difference between L1 and L2 so use L1
                                - but later: L1 loss better than MSE?
                                - theory: sparsity in representations? 
                        ---
                        ## Demucs (other considerations)
                        - uses a weight initialization scheme based on Fixup (for deep residual networks) to help even out weight magnitudes at deeper layers
                        - issues with input offsets (in samples)
                            - implies separation starts at beginning of tracks
                            - LSTM - accumulating errors? (Luo and Mesgarani, 2019)
                            - no issues with Conv-Tasnet
                            - Demucs solution: shift-trick: shift s samples, average output shifted -s samples (+ 0.3 dB SDR)
                        ---
                        ## Demucs Training
                        - Datasets
                            - MusDB (84/16/50 = 150)
                            - extra "raw stems" (150/x/x/ = 150)
                        - Metrics
                            - SDR (median)
                        - Training
                            - epoch (360x): all 11-second song snippets with 1 sec stride, 0-1 sec time shift
                                - extra (240x): 2-second snippets
                            - augmentation: tempo/pitch shift (+ 0.5 dB SDR, not on extra), multiplication of source, making new mixes, swapping channels
                            - Time 8 V100 GPUS, 16 GB RAM, (Conv-Tasnet 16, 32 GB ram)
                            - batch size: 64
                            - ADAM
                        ---
                        ## Demucs Results (SDR)
                        ![archpic](res/demucs-chart1.png)
                        ---
                        ## Demucs Results (MOS)
                        ![archpic](res/demucs-mos1.png)
                        ![archpic](res/demucs-mos2.png)
                        ---
                        ## Some Takeaways
                        - in general, bass/drums handled better by raw waveforms, pitchier things (other/vocals) better by spectrogram
                            - (except by D3Net both with no extra data, but maybe less smearier, not captured by SDR)
                            - Conv-Tasnet - drums and bass - "hollow instrument attacks", artifacts
                            - STFT tradeoff: bigger windows, more smeary; smaller windows, not enough basis cosines;
                            - makes sense the pitchier instruments would benefit from spectrogram method,
                        - augmentation improved performance
                            - diversity of training sets?
                        - quantization (1014 MB to 120MB, no SDR loss), reduce number channels (64 to 32, -0.2 dB)
                        ---
                        ## Demucs Family
                        - Hybrid Demucs (2021)
                            - adds parallel branch with spectrogram
                            - improvement on other/vocals, but not SOTA at the time (beat by KUIELAB-MDX-Net on other/vocals)
                        - Hybrid Transformer Demucs (2022)
                            - replaces LSTM with Transformer
                            - sparse attention version has gains in 2 dB for drums/bass, 1+ dB gains for other/vocals
                                - limiting comparison pairs leading to sparse matrices
                        ---
                        ## Architecture (Hybrid Demucs)
                        ![archpic2](res/demucs-hybrid.png)

                        ---
                        ## Architecture (HT Demucs)
                        ![archpic](res/demucs-tx.png)

                        ---
                        ## References
                        - Choromanski, K. and Colwell, L. (2020). Rethinking Attention with Performers. Google Research. https://blog.research.google/2020/10/rethinking-attention-with-performers.html
                        - Défossez, A., Usunier, N., Bottou, L., and Bach., F. (2021). Music Source Separation in the Waveform Domain. https://arxiv.org/abs/1911.13254
                        - Défossez, A. (2021). Hybrid Spectrogram and Waveform Source Separation. https://arxiv.org/abs/2111.03600
                        - PyTorch Contributors (2023). PReLu. PyTorch. https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html
                        - Luo, Y. and Mesgarani, N. (2019). Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation. *IEEE/ACM Transactions on Audio, Speech, and Language Processing 27(8)*, 1256-1266. https://doi.org/10.1109/TASLP.2019.2915167
                        ---
                        ## References cont.
                        - Luo, Y. and Mesgarani, N. (2018). Tasnet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation. *IEEE International Conference on Acoustics, Speech, and Signal Processing*. https://doi.org/10.1109/ICASSP.2018.8462116 
                        - Rouard, S., Massa, F., and Défossez, A. (2023). Hybrid Transformers for Music Source Separation. *ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech, and Signal Processing*. https://doi.org/10.1109/ICASSP49357.2023.10096956
                        - Tovar, A. D. (2020). GLU: Gated Linear Unit implementation. Deep Learning made easy. https://medium.com/deeplearningmadeeasy/glu-gated-linear-unit-21e71cd52081 
                        ---
                        ## References cont.
                        - Vincent, E., Gribonval, R., and Févotte, C. (2006). Performance measurement in blind audio source separation. *IEEE Transactions on Audio, Speech, and Language Processing 14(4)*, 1462-1469. https://doi.org/10.1109/TSA.2005.858005
                        - Wang, C.F. (2018). A Basic Introduction to Separable Convolutions. Towards Data Sceience. https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728
                        - Wang, Z., Wang. X., Li., X., Fu., Q., and Yan., Y. (2016). Oracle Performance Investigation of the Ideal Masks. *2016 IEEE International Workshop on Acoustic Signal Enhancement*. https://doi.org/10.1109/IWAENC.2016.7602888

                    </textarea>

                </section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMath.KaTeX, RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
